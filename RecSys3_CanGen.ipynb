{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lapshinaaa/recsys-tasks/blob/main/RecSys3_CanGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEwEWoxzCW4a"
      },
      "source": [
        "# Recommender Systems\n",
        "## Candidate Generation\n",
        "\n",
        "Note from YSDA course: During practice, we've discussed common CG stage abstractions, quality metrics and implemented several models with a KNN candidate generator class. In homework your task will be to dive deeper into this domain and implement a few more models, compare them and come up with a way to use all of them simultaneously. The homework consists of several subtasks, which are independent, and two bonus subtasks.\n",
        "\n",
        "Let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X-Qv51XCW4h"
      },
      "source": [
        "### Imports and metrics setup\n",
        "We'll use the metrics and evaluator class, which we were using in practice about candidate generators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t9pGJWQ-CW4j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5838967-9e46-4bcc-bc2b-550f4012bd52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'recsys_course'...\n",
            "remote: Enumerating objects: 206, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 206 (delta 8), reused 5 (delta 4), pack-reused 189 (from 1)\u001b[K\n",
            "Receiving objects: 100% (206/206), 34.65 MiB | 36.21 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'recsys_course/week03/homework/requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mObtaining file:///content/recsys_course/grocery\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting catboost>=1.2.5 (from grocery==0.1.0)\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (1.5.2)\n",
            "Collecting matplotlib>=3.10.1 (from grocery==0.1.0)\n",
            "  Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy~=1.26.4 (from grocery==0.1.0)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m416.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: polars>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (1.25.2)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (2.32.4)\n",
            "Requirement already satisfied: scikit-learn>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.15.2 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (1.16.3)\n",
            "Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (4.67.1)\n",
            "Collecting voyager>=2.1.0 (from grocery==0.1.0)\n",
            "  Downloading voyager-2.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost>=1.2.5->grocery==0.1.0) (0.21)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost>=1.2.5->grocery==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost>=1.2.5->grocery==0.1.0) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost>=1.2.5->grocery==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->grocery==0.1.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->grocery==0.1.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->grocery==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->grocery==0.1.0) (2025.10.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->grocery==0.1.0) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost>=1.2.5->grocery==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost>=1.2.5->grocery==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost>=1.2.5->grocery==0.1.0) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading voyager-2.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: grocery\n",
            "  Building editable for grocery (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grocery: filename=grocery-0.1.0-py3-none-any.whl size=1469 sha256=2576628215fcdf0720d0610fe80d10536c20012dfd4e0f9bfa0fda8ee9f5348a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hi_io71b/wheels/c4/79/e6/303cf9aa697da62bfff01b4cd31de0aaf287d53dd8ba97d686\n",
            "Successfully built grocery\n",
            "Installing collected packages: numpy, voyager, matplotlib, catboost, grocery\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed catboost-1.2.8 grocery-0.1.0 matplotlib-3.10.7 numpy-1.26.4 voyager-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              },
              "id": "060f9c6e5eb34678a46853b383808994"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# if you are running this notebook in colab, datasphere, etc - you need to clone the repository first\n",
        "\n",
        "!git clone https://github.com/yandexdataschool/recsys_course\n",
        "!pip install -r recsys_course/week03/homework/requirements.txt\n",
        "!pip install -e recsys_course/grocery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZDgEQisSCW4n"
      },
      "outputs": [],
      "source": [
        "import numba as nb\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import scipy.sparse as sp\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from grocery.metrics import Evaluator, Recall, Novelty, CategoryDiversity\n",
        "from grocery.recommender.candidates import CandidateGenerator, Candidate\n",
        "from grocery.utils.dataset import build_matrix_with_mappings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse.linalg import inv"
      ],
      "metadata": {
        "id": "6KPBNHan22BR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y8fNRwdWCW4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b30794-07c5-4319-aa2d-294aa454b7fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "lavka.zip: 100%|██████████████████████████████████████████████████| 447M/447M [00:11<00:00, 37.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unpacking lavka.zip...\n",
            "Files from lavka.zip successfully unpacked\n",
            "\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = \"../../data/lavka\"\n",
        "\n",
        "from grocery.utils.dataset import download_and_extract\n",
        "download_and_extract(\n",
        "     url=\"https://www.kaggle.com/api/v1/datasets/download/thekabeton/ysda-recsys-2025-lavka-dataset\",\n",
        "     filename=\"lavka.zip\",\n",
        "     dest_dir=DATA_DIR\n",
        " )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0N3rbNmjCW4r"
      },
      "outputs": [],
      "source": [
        "data = (\n",
        "    pl.read_parquet(f\"{DATA_DIR}/train.parquet\")\n",
        "    .filter(pl.col(\"action_type\") == \"AT_CartUpdate\")\n",
        "    .select(\n",
        "        pl.col(\"user_id\"),\n",
        "        pl.col(\"product_id\").alias(\"item_id\"),\n",
        "        pl.col(\"request_id\"),\n",
        "        pl.lit(1).alias(\"rating\"),\n",
        "        pl.col(\"timestamp\"),\n",
        "        pl.col(\"product_category\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "# global timepoint split!\n",
        "train, test = train_test_split(data.sort(\"timestamp\"), test_size=0.2, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ON9QxWCSCW4s"
      },
      "outputs": [],
      "source": [
        "evaluator = Evaluator(\n",
        "    metrics=[\n",
        "        Recall(10),\n",
        "        Recall(50),\n",
        "        Novelty(train, 10),\n",
        "        Novelty(train, 50),\n",
        "        CategoryDiversity(train, 10),\n",
        "        CategoryDiversity(train, 50),\n",
        "    ]\n",
        ")\n",
        "\n",
        "evaluator.load_test_actions(\n",
        "    test.filter(pl.col(\"user_id\").is_in(train[\"user_id\"].unique()))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I6DKQ81CW4u"
      },
      "source": [
        "## Candidate Generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRWx8le-CW4v"
      },
      "source": [
        "### EASE (0.5 points), LinearItemToItemCG and ANN (1 points)\n",
        "Implement the model from [Embarrassingly Shallow Autoencoders for Sparse Data](https://arxiv.org/pdf/1905.03375).\n",
        "\n",
        "From practice we know that we can present our training data as a sparse matrix $X \\in \\mathbb{R}^{|U| x |I|}$, where each element $x_{ui}$ is a binary value, which is positive if the user $u$ had an interaction with item $i$, or rating from their interactions, or number of interactions.\n",
        "\n",
        "EASE suggests a linear model, that encodes all items as an item-item matrix $B$, and each item can be represented as a linear combination of other items.\n",
        "We consider two features. The first is categorical — the ID of the item being evaluated. The second group consists of items with which the user has had positive interactions (excluding the item in question). We then take the cross-product of these two groups to generate features like: [currently evaluating item i, item j appears in the user’s history]. We train a linear model on these cross-features. The target is 1 for positive interactions and 0 otherwise (even if the user hasn’t been shown the item).\n",
        "\n",
        "We use MSE loss and L2 regularization (a convex loss function), adding the constraint that diagonal weights must be zero. This constraint, $diag(B) = 0$, is crucial as to avoid the trivial solution $B = I$ (self-similarity of items), where $I$ is the identity matrix.\n",
        "\n",
        "The optimization problem in matrix form is as follows:\n",
        "\n",
        "$$\\min_{B} ||X - XB||^2_F + \\lambda ||B||^2_F$$\n",
        "$$\\text{s.t. diag}(B) = 0$$\n",
        "\n",
        "Turns out, the problem has an exact analytical solution - your task is to derive it and implement!\n",
        "\n",
        "*Notes: it's better to implement both sparse with scipy.sparse.linalg.inv approach and dense with np.linalg.inv, and to check if dense matrix fits in your memory :-)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GKIbyj6QCW4x"
      },
      "outputs": [],
      "source": [
        "class EASE:\n",
        "    def __init__(self, lambd: float = 0.1):\n",
        "        self.lambd = lambd\n",
        "        self.B = None\n",
        "        self.item_id2idx = None # column index -> item_id\n",
        "        self.item_idx2id = None # item_id -> column index\n",
        "\n",
        "    def fit(self, ratings: pl.DataFrame, sparse_inv: bool = False):\n",
        "        X, (user_id2idx, item_id2idx, user_idx2id, item_idx2id) = build_matrix_with_mappings(ratings)\n",
        "        self.item_id2idx = item_id2idx\n",
        "        self.item_idx2id = item_idx2id\n",
        "\n",
        "        # Gram Matrix G (according to the article) = X^T X\n",
        "        G = X.T @ X  # shape: (num_items x num_items), sparse\n",
        "\n",
        "        # λ * I for regularization\n",
        "        num_items = G.shape[0]\n",
        "        G = G + self.lambd * sp.eye(num_items, format=\"csc\")\n",
        "\n",
        "        P = inv(G)  # returns sparse linear operator\n",
        "        P = P.toarray()  # convert to dense (safe because only num_items x num_items)\n",
        "\n",
        "        #  B\n",
        "        B = -P / np.diag(P)  # broadcasting division by diagonal\n",
        "        np.fill_diagonal(B, 0)  # enforce diag(B)=0\n",
        "\n",
        "        self.B = B\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58bJUgHWCW4y"
      },
      "source": [
        "Now let's implement a class to use that model for extracting candidates (or straight-up recommending) for a certain user.\n",
        "\n",
        "Some notes:\n",
        "- Do not forget about the fact that model operates in indices, but CG should receive user IDs and return `Candidates` with item IDs (not indices!)\n",
        "- Utilize the fact that user-item interactions is a sparse matrix, meaning there are only few positive elements, so you can try to iterate over them, extract needed weights from sparse matrix and sum them.\n",
        "- You may not even store the user-item matrix, but receive a dict with user history as a list of IDs during `__init__.py`. Although this is not a part of this homework, such class would later allow you to switch the dict for some other user history storage (or maybe even a generator, yeilding results from a database, .etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VOJmGE6cCW4z"
      },
      "outputs": [],
      "source": [
        "class LinearItemToItemCG(CandidateGenerator):\n",
        "    def __init__(self,\n",
        "                 B: sp.spmatrix | np.ndarray,\n",
        "                 user_id2idx: dict[int, int],\n",
        "                 item_id2idx: dict[int, int],\n",
        "                 X: sp.spmatrix | np.ndarray,\n",
        "                #  history: dict[int, list[int]],\n",
        "                 ):\n",
        "        \"\"\"Item to Item Linear Model applier.\n",
        "\n",
        "        Args:\n",
        "            B (sp.sparse.spmatrix | np.ndarray): Weight matrix.\n",
        "            user_id2idx (dict[int, int]): Mapping from user IDs to indices.\n",
        "            item_id2idx (dict[int, int]): Mapping from item IDs to indices.\n",
        "            X (sp.sparse.spmatrix | np.ndarray): User-item interaction matrix.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.B = B                          # (num_items × num_items)\n",
        "        self.user_id2idx = user_id2idx      # map user_id → row index in X\n",
        "        self.item_id2idx = item_id2idx      # map item_id → column index in X\n",
        "        self.idx2item_id = {v: k for k, v in item_id2idx.items()} # reversing the mapping here\n",
        "        self.X = X\n",
        "\n",
        "\n",
        "    def extract_candidates(self, object_id: int, n: int = 10) -> list[Candidate]:\n",
        "\n",
        "        if object_id not in self.user_id2idx: # edge case user does not exist\n",
        "          return []\n",
        "\n",
        "        user_idx = self.user_id_2idx[object_id]\n",
        "        user_vector = self.X[user_idx] # getting the user history as a sparse row\n",
        "\n",
        "        # now we're going to score the items and remove the ones the user has already interacted with\n",
        "        scores = user_vector @ self.B\n",
        "        scores = np.asarray(scores).ravel()\n",
        "        seen_items = set(self.X[user_idx].indices)\n",
        "        scores[list(seen_items)] = -np.inf # we're not recommending what has already been seen\n",
        "\n",
        "        # select top items\n",
        "        top_indices = np.argpartition(-scores, n)[:n]\n",
        "        top_indices = top_indices[np.argsort(-scores[top_indices])]\n",
        "\n",
        "        return [Candidate(id=self.idx2id[idx]) for idx in top_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRRL1lc_CW40"
      },
      "source": [
        "Your model should meet the quality level needed - for defined in this notebook train dataset, initialization and default hyperparameters from model class template, reference implementation achieves:\n",
        "\n",
        "$$recall@10 >= 0.04$$\n",
        "$$recall@50 >= 0.15$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QDSCaETCW41"
      },
      "outputs": [],
      "source": [
        "model = EASE(lambd=0.1)\n",
        "model.fit(train)\n",
        "\n",
        "ease_cg = LinearItemToItemCG(model.B, model.item_id2idx, model.item_id2idx, X)\n",
        "\n",
        "metrics = evaluator.evaluate(lambda user_id, n: ease_cg.extract_candidates(user_id, n))\n",
        "for metric_name, metric_value in metrics.items():\n",
        "    print(f\"{metric_name} = {metric_value:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1UqSjCJCW42"
      },
      "source": [
        "Now let's implement ANN candgen and compare searching in ALS embedding space with EASE scoring. There are lots of options to choose from, but we recommend settling on `faiss` or `voyager` libraries. The latter utilizes HNSW algorithm and has `ef_construction` parameter when building and `query_ef` parameter in `query` method, they control the tradeoff between speed and recall - feel free to experiment a bit with them.\n",
        "\n",
        "Here I could say *\"you can write your own HNSW\"*, but this is breaking a butterfly upon the wheel - please don't. The fact that ANN is used instead of KNN is enough optimization :D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-C7SZIsCW42"
      },
      "outputs": [],
      "source": [
        "from grocery.models import ALS\n",
        "from grocery.recommender.candidates import DotProductKNN\n",
        "\n",
        "def extract_embeddings(model: ALS) -> tuple[dict[int, np.array], dict[int, np.array]]:\n",
        "    return (\n",
        "        {ID: model.user_vectors[idx] for ID, idx in model.user_id2idx.items()},\n",
        "        {ID: model.item_vectors[idx] for ID, idx in model.item_id2idx.items()}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6AcPEwoCW43"
      },
      "outputs": [],
      "source": [
        "from voyager import Index, Space\n",
        "# what space do you need to use?\n",
        "\n",
        "\n",
        "class ANN(CandidateGenerator):\n",
        "    def __init__(self,\n",
        "                 left_embeddings: dict[int, np.array],\n",
        "                 right_embeddings: dict[int, np.array]):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: your code here\n",
        "\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def build_index(vectors: np.ndarray, ids: list[int]):\n",
        "\n",
        "        # TODO: your code here\n",
        "\n",
        "        pass\n",
        "\n",
        "    def extract_candidates(self, object_id: int, n: int = 10) -> list[Candidate]:\n",
        "\n",
        "        # TODO: your code here\n",
        "\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "608gmmgGCW44"
      },
      "source": [
        "Compare the resulting class with DotProductKNN from practice: speed, recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPTDxyQ2CW45"
      },
      "outputs": [],
      "source": [
        "als = ALS(max_iter=100)\n",
        "als.fit(train)\n",
        "\n",
        "user_embeddings, item_embeddings = extract_embeddings(als)\n",
        "als_ann = ANN(user_embeddings, item_embeddings)\n",
        "als_knn = DotProductKNN(user_embeddings, item_embeddings)\n",
        "\n",
        "# TODO: your code here\n",
        "# compare speed and recall, better to write your thoughts as md cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x08ZjYloCW46"
      },
      "source": [
        "### SLIM (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmzA8sTRCW47"
      },
      "source": [
        "The next model in our arsenal will be **S**parse **Li**near **M**ethod a.k.a. [SLIM](https://ieeexplore.ieee.org/document/6137254) (PDF is available [here](https://www.researchgate.net/profile/George_Karypis/publication/220765374_SLIM_Sparse_Linear_Methods_for_Top-N_Recommender_Systems/links/549ee9ac0cf257a635fe7010.pdf)).\n",
        "\n",
        "Actually, SLIM was introduced long before EASE, and EASE cites SLIM (and uses a lot of it as a foundation), not the other way around. For educational purposes, we switched their order of presentation in the homework. The algorithm solves the same problem of finding best item-item weight matrix, but with different loss - the main difference is L1 regularization for sparsity component, which helps the algorithm to build a sparse weight matrix, which is both memory-efficient and compute-efficient, if you organize your storage accordingly.\n",
        "\n",
        "\n",
        "The optimization problem:\n",
        "\n",
        "$$\\min_{B} \\frac{1}{2}||X - XB||^2_F + \\frac{\\beta}{2} ||B||^2_F + \\lambda ||B||_F$$\n",
        "$$\\text{subject to } B \\geq 0 \\text{, diag}(B) = 0$$\n",
        "\n",
        "This is solvable with coordinate descent by projecting each individual weight (non-negative constraint) and manually setting diagonal weights to zero during all the steps. Moreover, each item weight vector is independent from others, so the task is easily parallelizable across items. However, there aren't any optimizers that can do that out of the box. As proposed in this [paper](https://www.slideshare.net/slideshow/efficient-slides/27138952), we'll simplify the problem a bit by dropping the $diag(B) = 0$ constraint and setting the item column of the interaction matrix to zero when fitting for it. Let's see, whether it's still a viable model - simple scikit-learn should do the trick.\n",
        "\n",
        "Implement SLIM model using sklearn ElasticNet solver and optionally multiprocessing. The steps are as follows:\n",
        "- Build sparse matrix of interactions\n",
        "- For each item $u$:\n",
        "    - Get interaction matrix $X$ - the features\n",
        "    - Extract target vector $y = X_u \\in R^{|U|}$ - all interactions with this item\n",
        "    - Find solution and save it\n",
        "- Extract sparse coefs to one weight matrix B\n",
        "\n",
        "*Notes:* multiprocessing and jupyter notebooks are not really friends, so if you want to speed up the model training by parallelization - move it into a separate file, save matrix to `.npy` and launch it from here using Ipython magic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9bC_TFRCW48"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "\n",
        "def _solve_item(X, item_idx, alpha, l1_ratio):\n",
        "\n",
        "    # TODO: your code here\n",
        "\n",
        "    # X, y = ...\n",
        "    # solver = ElasticNet(...)\n",
        "    # solver.fit(...)\n",
        "    # coef = ...\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "class SLIM:\n",
        "    def __init__(self,\n",
        "                 alpha: float = 0.1,\n",
        "                 l1_ratio: float = 0.01,\n",
        "                 num_processes: int | None = None,\n",
        "                 ):\n",
        "\n",
        "        # TODO: your code here\n",
        "\n",
        "        pass\n",
        "\n",
        "    def fit(self, ratings: pl.DataFrame):\n",
        "\n",
        "        # TODO: your code here\n",
        "\n",
        "        # self.B = sp.sparse.matrix or smth like that\n",
        "        # ... = build_matrix_with_mappings(ratings)\n",
        "        # for item in range(num_items):\n",
        "        #     self.B[item] = _solve_item(...)\n",
        "\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAyZXnXtCW48"
      },
      "outputs": [],
      "source": [
        "slim = SLIM(num_processes=1)\n",
        "slim.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T4czpX8CW49"
      },
      "source": [
        "Your model should meet the quality level needed - for defined in this notebook train dataset, initialization and default hyperparameters from model class template, reference implementation achieves:\n",
        "\n",
        "$$recall@10 >= 0.04$$\n",
        "$$recall@50 >= 0.12$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uhZULQICW4-"
      },
      "outputs": [],
      "source": [
        "slim_cg = LinearItemToItemCG(...)\n",
        "\n",
        "# TODO: your code here\n",
        "# compare speed and recall, better to write your thoughts as md cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mgpf71aCW4_"
      },
      "source": [
        "#### Bonus task\n",
        "If you're really interested in implementing the models from scratch, you can try to implement your own SLIM using a lower-level language with parallelization. Here are some hints on how you can build and speed up your implementation.\n",
        "\n",
        "The proposed way is to use the coordinate descent, so if we fix item $i$ - we can solve the task independently:\n",
        "\n",
        "$$ L_i = \\frac{1}{2} || a_i - \\sum_j w_{ij}a_j || ^ 2 + \\lambda \\sum_j |w_{ij}| + \\beta \\sum (w_{ij}^2) \\rightarrow \\min_{w_{i1}...w_{in}}$$\n",
        "\n",
        "where $a_j$ is j-th column of interaction matrix. Since we are solving for nonnegative weights, all the modules of weights can be removed.\n",
        "\n",
        "The step of coordinate descent consists of fixating all the weights except one and fully optimizing by one component. The equation for the weight optimum is dervied from solving the equation of gradient being equal to zero:\n",
        "\n",
        "$$w_{ik} = \\frac{\\langle a_i, a_k \\rangle - \\sum_{j \\ne k}w_{ij} \\langle a_j, a_k \\rangle - \\lambda}{||a_k||^2 + \\beta}$$\n",
        "\n",
        "For diagonal weight, we simply set the weight to zero, and for all the other weights we project in onto the feasible set:\n",
        "\n",
        "$$w_{ik} = ReLU \\left(\\frac{\\langle a_i, a_k \\rangle - \\sum_{j \\ne k}w_{ij} \\langle a_j, a_k \\rangle - \\lambda}{||a_k||^2 + \\beta} \\right)$$\n",
        "\n",
        "In case of binary interaction matrix, if you look closely, there are some iterations that you can skip because for certain $\\lambda$ value the constraints make the weights go to zero in any scenario. If $\\langle a_j, a_k \\rangle < \\lambda$ - you can skip the iterations with number $k$; if for item $i$ the number of interactions is less than $\\lambda$ - you can just skip this item altogether and set them to zero.\n",
        "\n",
        "Good luck, if you decide to tackle this! Working solution will get you up to 2 points, if you manage to present:\n",
        "- the source code (yours).\n",
        "- setup and run instructions.\n",
        "- resulting recall (you can get it from loading the matrix into **LinearItemToItemCG** class)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vICJV8QJCW5A"
      },
      "source": [
        "### BPR with popularity sampling (1.5 points)\n",
        "\n",
        "Now let's try to improve the matrix factorization models. In practice we tested alternating least squares and logistic factorization, which were trying to solve an optimization problem of predicting the correct score. But in recommender systems, we usually need to rank the items, and that is exactly what [BPR: Bayesian Personalized Ranking from Implicit Feedback](https://arxiv.org/pdf/1205.2618) does. Instead of predicting explicit ratings, BPR assumes that a user prefers items with which they have interacted over those they have not. It optimizes a pairwise ranking objective to ensure that, for each user, positive (observed) items are ranked higher than negative (unobserved) items, using gradient-based optimization (typically SGD, but there implementations that allow RMSProp, Adam, etc).\n",
        "\n",
        "The BPR loss aims to maximize the probability that for a given user $u$, a positively interacted item $i$ is ranked higher than a negative item $j$. So, for a triplet $u, i, j$ the loss is defined as:\n",
        "\n",
        "$$L(u, i, j) = -\\ln \\sigma(\\hat{x}_{ui} - \\hat{x}_{uj}) + \\lambda ||\\Theta||^2$$\n",
        "\n",
        "where $\\Theta$ represents parameters and adds for regularization. In our task, you'll be using a matrix factorization approach with user biases:\n",
        "\n",
        "$$\\hat{x}_{ui} = \\langle p_u, q_u \\rangle + b_{i}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H502Ah-KCW5B"
      },
      "source": [
        "##### Optimizer step\n",
        "\n",
        "First, implement the optimizer step for a triplet $u, i, j$ - this will be your mini-batch in SGD optimization. For sampled indices, update the user embedding, item embedding and item bias. The derivation of the gradients for parameters is left to you (and is relatively easy to do).\n",
        "\n",
        "Both in this and the next task reference solution contains some parts, that are JIT-compiled using `Numba` and seems to be working significantly faster. It is recommended to also adapt your code so that it's suitable for `Numba` (you'll have the templates for functions), but if you're lazy and patient - you can just send a pure Python version. But you still need to fully train the model at least once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wWGY91tCW5C"
      },
      "outputs": [],
      "source": [
        "@nb.njit\n",
        "def optimizer_step(user_vectors, item_vectors, user_idx, pos_item_idx, neg_item_idx, lr, lambd):\n",
        "    \"\"\"Optimization step for BPR model\n",
        "    1. Calculate the dot-product scores for the positive and negative items.\n",
        "    2. Apply sigmoid function to the difference of the scores to get P(u, i > j).\n",
        "    3. Calculate the gradients for the user and item vectors.\n",
        "    4. Update inplace the user and item vectors.\n",
        "\n",
        "    Args:\n",
        "        user_vectors (np.ndarray[n_users, dim]): User embeddings.\n",
        "        item_vectors (np.ndarray[n_items, dim]): Item embeddings.\n",
        "        user_idx (int): User index.\n",
        "        pos_item_idx (int): Positive item index.\n",
        "        neg_item_idx (int): Negative item index.\n",
        "        lr (float): Learning rate.\n",
        "        lambd (float): Regularization coefficient.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: your code here\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPpdA-4VCW5C"
      },
      "outputs": [],
      "source": [
        "def test_bpr_optimizer(step_function):\n",
        "    user_embeddings = np.array([\n",
        "        [-0.004, -0.01 , -0.012,  0.038,  0.054],\n",
        "        [ 0.012,  0.038,  0.   , -0.06 ,  0.015]\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    item_embeddings = np.array([\n",
        "        [ 0.034,  0.047,  0.061, -0.043, -0.017],\n",
        "        [-0.005,  0.065, -0.086,  0.055, -0.013]\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    gt_user_embeddings = np.array([\n",
        "        [-0.00200446, -0.01080256, -0.00450913,  0.03270608,  0.05325943],\n",
        "        [ 0.012     ,  0.038     ,  0.        , -0.06      ,  0.015     ],\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    gt_item_embeddings = np.array([\n",
        "        [ 0.03345943,  0.04602858,  0.0597883,  -0.04066461, -0.01412233],\n",
        "        [-0.00474943,  0.06485142, -0.0845383,   0.0525446,  -0.01557767],\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    optimizer_step(user_embeddings, item_embeddings, 0, 0, 1, 0.1, 0.1)\n",
        "\n",
        "    assert np.allclose(user_embeddings, gt_user_embeddings), f\"User embeddings are not close to the ground truth\"\n",
        "    assert np.allclose(item_embeddings, gt_item_embeddings), f\"Item embeddings are not close to the ground truth\"\n",
        "\n",
        "    user_embeddings = np.array([\n",
        "        [1, 0, 0, 0, 0],\n",
        "        [1, 1, -1, -1, 0]\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    item_embeddings = np.array([\n",
        "        [0, 1, 0, -1, 1],\n",
        "        [-1, 0, 1, 0, -1]\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    gt_user_embeddings = np.array([\n",
        "        [ 1.2689414 ,  0.26894143, -0.26894143, -0.26894143,  0.53788286],\n",
        "        [ 1.        ,  1.        , -1.        , -1.        ,  0.        ]\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    gt_item_embeddings = np.array([\n",
        "        [ 0.26894143,  1.        ,  0.        , -1.        ,  1.        ],\n",
        "        [-1.2689414 ,  0.        ,  1.        ,  0.        , -1.        ]\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    step_function(user_embeddings, item_embeddings, 0, 0, 1, 1, 0)\n",
        "\n",
        "    assert np.allclose(user_embeddings, gt_user_embeddings), f\"User embeddings are not close to the ground truth\"\n",
        "    assert np.allclose(item_embeddings, gt_item_embeddings), f\"Item embeddings are not close to the ground truth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XgglAyDCW5D"
      },
      "outputs": [],
      "source": [
        "test_bpr_optimizer(optimizer_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRZdLVp_CW5E"
      },
      "source": [
        "##### Sampling\n",
        "The next step is to implement the negative sampling. We will be storing IDs in the same format, as they are in dataset - just two arrays of IDs, where I-th interaction was between `user_ids[i]` and `item_ids[i]`. This will allow us to sample both users and items based on their \"popularity\" - the more times item or user was interacting with something, more frequently it will pop up in triplets. It's a good heuristic and works well in BPR.\n",
        "\n",
        "In proposed implementation, we will be sampling any item as negative and then checking if it's actually negative with lookup into all user positives. Because there are a lot of items and dataset is sparse, the chance of sampling a fake negative is not really high, so we won't be slipping too much. Depending on implementation, storing all the positives, negatives and sampling from them may be slower or faster, but this one is definitely fast!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzWv8_mgCW5E"
      },
      "outputs": [],
      "source": [
        "@nb.njit\n",
        "def sample_triplet_popularity(item_ids: np.ndarray, user_ids: np.ndarray):\n",
        "    \"\"\"Sample triplet using fast popularity sampling from whole dataset, represented\n",
        "    as two arrays of item and user IDs for each positive interaction from the dataset.\n",
        "    1. Sample random interaction of the dataset\n",
        "    2. Get user ID, positive item ID from this interaction.\n",
        "    3. Sample random interaction of the dataset as negative item.\n",
        "    The approach does not guarantee that the negative item is indeed negative for a user.\n",
        "\n",
        "    Args:\n",
        "        item_ids (np.ndarray[n_samples]): array of item IDs, each point represents a positive interaction from the dataset.\n",
        "        user_ids (np.ndarray[n_samples]): array of user IDs, each point represents a positive interaction from the dataset.\n",
        "\n",
        "    Returns:\n",
        "        tuple[int, int, int]: user ID, positive item ID, negative item ID.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: your code here\n",
        "    # return user_id, positive_id, negative_id\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgNBU-F3CW5F"
      },
      "outputs": [],
      "source": [
        "def test_sample_triplet_popularity(sampler, num_samples=100000):\n",
        "    item_ids = [4, 1, 2, 4, 3, 3, 4, 4, 2, 3]\n",
        "    user_ids = [1, 1, 2, 2, 3, 3, 4, 4, 3, 3]\n",
        "    # ratings data would look something like this\n",
        "    # pl.DataFrame({\n",
        "    #     \"user_id\": [4, 1, 2, 4, 3, 3, 4, 4, 5, 5],\n",
        "    #     \"item_id\": [1, 1, 2, 2, 3, 3, 4, 4, 5, 5],\n",
        "    # })\n",
        "\n",
        "    sampled_triplets = [sampler(item_ids, user_ids) for _ in range(num_samples)]\n",
        "    users, positives, negatives = zip(*sampled_triplets)\n",
        "\n",
        "    gt_user_freqs = np.bincount(user_ids) / len(user_ids)\n",
        "    gt_item_freqs = np.bincount(item_ids) / len(item_ids)\n",
        "    user_freqs = np.bincount(users) / len(users)\n",
        "    pos_item_freqs = np.bincount(positives) / len(positives)\n",
        "    neg_item_freqs = np.bincount(negatives) / len(negatives)\n",
        "\n",
        "    assert np.allclose(gt_user_freqs, user_freqs, atol=0.01)\n",
        "    assert np.allclose(gt_item_freqs, pos_item_freqs, atol=0.01)\n",
        "    assert np.allclose(gt_item_freqs, neg_item_freqs, atol=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnEo6T5WCW5H"
      },
      "outputs": [],
      "source": [
        "test_sample_triplet_popularity(sample_triplet_popularity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lVeKGDzCW5H"
      },
      "source": [
        "##### The actual model\n",
        "Some code hints are left as comments, and docstring should provide the needed guidelines on how to implement the methods. Feel free to change the implementation if you want, as long as it's still BPR and it meets the quality benchmarks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb_coRFoCW5I"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class PopularityBPR:\n",
        "    def __init__(self,\n",
        "                 dim: int = 128,\n",
        "                 max_iter: int = 100,\n",
        "                 learning_rate: float = 0.01,\n",
        "                 lambd: float = 0.01,\n",
        "                ):\n",
        "        self.dim = dim\n",
        "        self.max_iter = max_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def _init_parameters(self, ratings: pl.DataFrame):\n",
        "        \"\"\"Initialize parameters for the model.\n",
        "        1. Build mappings from user and item IDs to indices\n",
        "        2. Initialize user and item vectors with scaled normal distribution\n",
        "        3. Initialize item biases with zeros\n",
        "        4. Set item biases to zero if there are no interactions for an item\n",
        "        5. Set user vectors to zero if there are no interactions for a user\n",
        "\n",
        "        Args:\n",
        "            ratings (pl.DataFrame): pl.DataFrame with ratings\n",
        "        \"\"\"\n",
        "        R, mappings = build_matrix_with_mappings(ratings, additive=False)\n",
        "        self.user_id2idx, self.item_id2idx, self.user_idx2id, self.item_idx2id = mappings\n",
        "        self.num_samples = len(ratings)\n",
        "        self.n_users = len(self.user_id2idx)\n",
        "        self.n_items = len(self.item_id2idx)\n",
        "        self.item_biases = np.zeros((self.n_items, 1))\n",
        "        self.user_vectors = np.random.normal(loc=0, scale=0.2 / self.dim, size=(self.n_users, self.dim))\n",
        "        self.item_vectors = np.random.normal(loc=0, scale=0.2 / self.dim, size=(self.n_items, self.dim))\n",
        "        user_has_no_interactions = R.sum(axis=1) == 0\n",
        "        item_has_no_interactions = R.sum(axis=0) == 0\n",
        "        self.item_vectors[item_has_no_interactions] = 0\n",
        "        self.user_vectors[user_has_no_interactions] = 0\n",
        "\n",
        "\n",
        "    def _init_sampler(self, ratings: pl.DataFrame) -> tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Convert ratings to the format, awaited by the sampling function - meaning\n",
        "        np.arrays of item and user IDs for each positive interaction from the dataset.\n",
        "        Also save the user positives in any lookup-friendly data structure for checking,\n",
        "        whether and item is negative or positive for a user to skip it after sampling.\n",
        "\n",
        "        Args:\n",
        "            ratings (pl.DataFrame): Ratings data in standard format.\n",
        "        Returns:\n",
        "            tuple[np.ndarray, np.ndarray]: Item IDs and user IDs.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: your code here\n",
        "\n",
        "        pass\n",
        "\n",
        "    def train_one_epoch(self):\n",
        "        \"\"\"Train one epoch of the model and compute the number of skipped triplets.\n",
        "        For num_samples times:\n",
        "        1. Sample a triplet (user, positive item, negative item)\n",
        "        2. If the negative item is not a true negative for the user, skip the triplet\n",
        "        3. Otherwise, update the model parameters\n",
        "\n",
        "        Returns:\n",
        "            int: Number of skipped triplets.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: your code here\n",
        "\n",
        "        # skipped = 0\n",
        "        # for _ in range(self.num_samples):\n",
        "        #       triplet = sample_triplet()\n",
        "        #       if verify_triplet(triplet):\n",
        "        #           optimizer_step(triplet)\n",
        "        # return skipped\n",
        "\n",
        "        pass\n",
        "\n",
        "    def fit(self, ratings: pl.DataFrame):\n",
        "        \"\"\"Fit the model to the ratings data.\n",
        "        1. Initialize parameters and sampler\n",
        "        2. Train the model for max_iter epochs\n",
        "        3. Print the number of skipped triplets\n",
        "\n",
        "        Args:\n",
        "            ratings (pl.DataFrame): Ratings data in standard format.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: your code here\n",
        "\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djYVS4XaCW5J"
      },
      "outputs": [],
      "source": [
        "bpr = PopularityBPR(max_iter=50)\n",
        "bpr.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVTVsNk0CW5K"
      },
      "source": [
        "For defined in this notebook train dataset, initialization and default hyperparameters from model class template, reference implementation achieves:\n",
        "\n",
        "$$recall@10 >= 0.07$$\n",
        "$$recall@50 >= 0.18$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZojGMM0CW5b"
      },
      "outputs": [],
      "source": [
        "user_embeddings, item_embeddings = extract_embeddings(bpr)\n",
        "knn = DotProductKNN(user_embeddings, item_embeddings)\n",
        "\n",
        "# TODO: your code here\n",
        "# compare speed and recall, better to write your thoughts as md cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIVM-1xVCW5c"
      },
      "source": [
        "### BPR with adaptive sampling (1.5 points)\n",
        "\n",
        "Now let's spice things up a bit!\n",
        "\n",
        "Popularity sampling is fast and effective, but eventually it converges to a state where it primarily samples easy, obvious negatives and does not learn anything new. This can be fixed by using hard negatives - the ones, that are negative for user, but have high model score. We'll do this by changing popularity sampling to weighted sampling, where weights are proportionate to model confidence.\n",
        "\n",
        "For positive weights, we'll compute the probabilites as frequencies in dataset for each user (or any other way if you want to). For first `num_epochs_without_updates` we'll set the negative weights uniform, after that we'll compute the scores and softmax them with temperature to get the sampling probabilities. The higher the score - the more frequently we will be sampling these negatives.\n",
        "\n",
        "Since we're using complex weighted sampling (three random generator calls in total), we'll ensure that both positive and negative weights are correct, meaning that negative items have zero weight in positive sampling weights and vice versa. This should be done both in sampler initialization and during weights update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvM9g0QFCW5d"
      },
      "outputs": [],
      "source": [
        "@nb.njit\n",
        "def nb_random_choice(arr, prob):\n",
        "    \"\"\"Analogue of weighted np.random.choice (Numba-compatible).\n",
        "    Args:\n",
        "        arr (np.ndarray): Array of items to sample from.\n",
        "        prob (np.ndarray): Array of probabilities for each item.\n",
        "    Returns:\n",
        "        int: Index of the sampled item.\n",
        "    \"\"\"\n",
        "    return arr[np.searchsorted(np.cumsum(prob), np.random.random(), side=\"right\")]\n",
        "\n",
        "\n",
        "@nb.njit\n",
        "def weighted_sample_triplet(users, items, user_weights, positive_weights, negative_weights):\n",
        "    \"\"\"Weighted sampling of triplet (user, positive item, negative item).\n",
        "\n",
        "    Args:\n",
        "        users (np.ndarray): Array of user IDs.\n",
        "        items (np.ndarray): Array of item IDs.\n",
        "        user_weights (np.ndarray[n_users]): Array of user weights.\n",
        "        positive_weights (np.ndarray[n_users, n_items]): Array of positive item weights for each user.\n",
        "        negative_weights (np.ndarray[n_users, n_items]): Array of negative item weights for each user.\n",
        "\n",
        "    Returns:\n",
        "        tuple[int, int, int]: User ID, positive item ID, negative item ID.\n",
        "    \"\"\"\n",
        "    user_idx = nb_random_choice(users, user_weights)\n",
        "    positive_item_idx = nb_random_choice(items, positive_weights[user_idx])\n",
        "    negative_item_idx = nb_random_choice(items, negative_weights[user_idx])\n",
        "    return user_idx, positive_item_idx, negative_item_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irImvykRCW5e"
      },
      "outputs": [],
      "source": [
        "class AdaptiveBPR(PopularityBPR):\n",
        "    def __init__(self,\n",
        "                 dim: int = 128,\n",
        "                 max_iter: int = 100,\n",
        "                 learning_rate: float = 0.01,\n",
        "                 lambd: float = 0.01,\n",
        "                 weight_temperature: float = 0.5,\n",
        "                 num_epochs_without_updates: int = 10\n",
        "                ):\n",
        "        super().__init__(dim, max_iter, learning_rate, lambd)\n",
        "        self.t = weight_temperature\n",
        "        self.num_epochs_without_updates = num_epochs_without_updates\n",
        "\n",
        "    def _init_sampler(self, ratings: pl.DataFrame):\n",
        "        \"\"\"Initialize sampler weights.\n",
        "        There should be three weights, all proportional to popularity in dataset:\n",
        "        1. User weights - np.array[n_users]\n",
        "        2. Positive item weights - np.array[n_users, n_items]\n",
        "        3. Negative item weights - np.array[n_users, n_items]\n",
        "        Do not forget to normalize item weights after initialization inside each row and all user weights.\n",
        "\n",
        "        Args:\n",
        "            ratings (pl.DataFrame): Ratings data in standard format.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: your code here\n",
        "\n",
        "        # user_weights = [0, 0, 0, ...]\n",
        "        # positive_weights = [[0, 0, 0, ...], [0, 0, 0, ...], ...]\n",
        "\n",
        "        # for user, item in dataset:\n",
        "        #     user_weights[user] += 1\n",
        "        #     positive_weights[user, item] += 1\n",
        "\n",
        "        pass\n",
        "\n",
        "    def train_one_epoch(self):\n",
        "        \"\"\"\n",
        "        For num_samples times:\n",
        "        1. Sample a triplet (user, positive item, negative item) using weighted negative sampling\n",
        "        3. Update the model parameters\n",
        "\n",
        "        Returns:\n",
        "            int: Number of skipped triplets.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: your code here\n",
        "\n",
        "        pass\n",
        "\n",
        "    def update_weights(self):\n",
        "        \"\"\"Update negative item weights based on model scores.\n",
        "        1. Compute the scores for all items for each user.\n",
        "        2. Set to zero the weights for positive items, that are not negative for the user.\n",
        "        3. Apply softmax with temperature to the negative weights.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: your code here\n",
        "\n",
        "        # scores = user_emb @ item_emb\n",
        "        # negative_mask = self.negative_weights > 0\n",
        "        # negative_weights = softmax(scores[negative_mask], t)\n",
        "\n",
        "        pass\n",
        "\n",
        "    def fit(self, ratings: pl.DataFrame):\n",
        "        \"\"\"Fit the model to the ratings data.\n",
        "        - First initialize parameters\n",
        "        - Initialize sampler and weights\n",
        "        - For max_iter epochs:\n",
        "            - Train the model for one epoch\n",
        "            - If epoch >= num_epochs_without_updates, update the weights\n",
        "\n",
        "        Args:\n",
        "            ratings (pl.DataFrame): Ratings data in standard format.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: your code here\n",
        "\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVkcBAaRCW5f"
      },
      "outputs": [],
      "source": [
        "bpr = AdaptiveBPR()\n",
        "bpr.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We3QsDoGCW5g"
      },
      "source": [
        "For defined in this notebook train dataset, initialization and default hyperparameters from model class template, reference implementation achieves:\n",
        "\n",
        "$$recall@10 >= 0.09$$\n",
        "$$recall@50 >= 0.20$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCUofYMFCW5g"
      },
      "outputs": [],
      "source": [
        "user_embeddings, item_embeddings = extract_embeddings(bpr)\n",
        "knn = DotProductKNN(user_embeddings, item_embeddings)\n",
        "\n",
        "# TODO: your code here\n",
        "# compare speed and recall, better to write your thoughts as md cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXZZllfdCW5h"
      },
      "source": [
        "### Quality benchmark (0.5 points)\n",
        "\n",
        "This part is supposed to inspire you to be creative!\n",
        "\n",
        "Come up with a benchmark setup, where you compare:\n",
        "- The speed of model training on different dataset sizes (at least three).\n",
        "- The memory of stored CGs (ANN index, `.npz` file or sparse/dense I2I matrix).\n",
        "- The speed of CGs on inference (request per second in non-batched format).\n",
        "- The $recall@10$ and $recall@50$ of the algorithms you got.\n",
        "\n",
        "Each metric gets you 0.1 point, except model training speed which gets you 0.2 points. Present your results in a human-readable format, preferably using pandas/polars DataFrames or various plots within the notebook. Reports in an unreadable format may receive fewer points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDUG5StaCW5i"
      },
      "outputs": [],
      "source": [
        "# TODO: your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2fhAuCPCW5i"
      },
      "source": [
        "## Bonus task: MixiGen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN-NQpZiCW5j"
      },
      "source": [
        "MixiGen is Yandex's approach to create personalized adaptive blending of candidate generators.\n",
        "\n",
        "As we were discussing in practice, optimizing for recall is not always the best solution in terms of choosing the candidate generators and how much to extract from them - for example, you can learn to predict, whether the later-stage rankers will \"like\" the item from this candidate generator. The like can be defined as expected score, position in final ranking or binary marker that item is in top-N of all the candidates.\n",
        "\n",
        "But we want to use it to estimate, how much items do we need to extract from CG - we can't just \"rank\" them using item-dependent features. Moreover, we only know user features, candidate generator information, context information (time, place, etc) - but it's actually enough, if you add one other feature: position of item in candidate list from CG. This way, before extracting any candidates, we can run the following procedure:\n",
        "- Extract all the user and context features.\n",
        "- Create a sample for each CG and fill it with CG features and user/context ones.\n",
        "- Duplicate them and fill in the \"position in cg\" feature with all the possible options.\n",
        "- Score and rerank.\n",
        "\n",
        "Then we have a ranked list, where each sample contains a CG, from which we should extract another candidate. We can simply cutoff top-N of this list to get the number of candidates from each CG that we need to build the best N candidate list.\n",
        "\n",
        "It's better to use a listwise ranking loss for this task, because candidates depend on each other inside each request - more on this will be on week04, so you can postpone the bonus task, but any additional knowledge isn't required to solve it.\n",
        "\n",
        "In fact, this naive way will be pretty slow, but you can optimize it by not scoring a hundred candidates for each CG, but using [k-way merge](https://en.wikipedia.org/wiki/K-way_merge_algorithm) by iteratively scoring next candidate from CG, from which you pulled the last, or doing it in batches of ten for maximum speed, since extracting one candidate is not really standard operation. This is correct, if we assume that score is monotously decreasing, when position in candidate generator is increasing. Our main goal for this bonus will be to implement the model and see if the assumption is correct!\n",
        "\n",
        "For more details (or a clearer explanation) you can read the description of the algorithm from one of it's authors [here](https://t.me/WazowskiRecommends/59) and [here](https://t.me/WazowskiRecommends/61)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTDRUIR2CW5j"
      },
      "source": [
        "### What do you have to do\n",
        "For training mixigen, we need a ranker model - the code to compute the features and train a really simple catboost model. We also need a training dataset - logged candidate lists for several CGs with item and user features.\n",
        "\n",
        "1. Train the ranker (and optionally add more features)\n",
        "2. Train the models and create CGs\n",
        "3. For each request, which has at least one positive item, in mixigen training subset call each CG and log top-100 candidates\n",
        "4. For each logged candidate, join item and user features and score them using ranker model\n",
        "5. Define the target for mixigen - whether the item was in top-N for the request.\n",
        "6. Train the MixiGen model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QZ1P_vwCW5k"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQNjDWwVCW5l"
      },
      "outputs": [],
      "source": [
        "# !cp -r recsys_course/week03/homework/train_cbm_ranker.py train_cbm_ranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K40NFWRjCW5m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from catboost import CatBoostRanker, Pool\n",
        "\n",
        "# user_features and item_features return dataframes with conversion and cart update rates by user and item\n",
        "from train_cbm_ranker import train_cbm_ranker, user_features, item_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaulUXWwCW5n"
      },
      "outputs": [],
      "source": [
        "# we'll use a smaller subset of data, you can reduce it even further to fit in your RAM\n",
        "data, _ = train_test_split(\n",
        "    pl.read_parquet(f\"{DATA_DIR}/train.parquet\").sort(\"timestamp\"),\n",
        "    test_size=0.2,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "train_models_data, train_ranker_data = train_test_split(\n",
        "    data.sort(\"timestamp\"), test_size=0.5, shuffle=False\n",
        ")\n",
        "\n",
        "# train and save the ranker model\n",
        "if not os.path.exists(\"ranker.cbm\"):\n",
        "    ranker = train_cbm_ranker(train_models_data, train_ranker_data)\n",
        "    ranker.save_model(\"ranker.cbm\")\n",
        "else:\n",
        "    ranker = CatBoostRanker()\n",
        "    ranker.load_model(\"ranker.cbm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jx_v8CMCW5n"
      },
      "outputs": [],
      "source": [
        "# convert data to format for CG model training\n",
        "mlm_format_data = (\n",
        "    data\n",
        "    .filter(pl.col(\"action_type\") == \"AT_CartUpdate\")\n",
        "    .select(\n",
        "        pl.col(\"user_id\"),\n",
        "        pl.col(\"product_id\").alias(\"item_id\"),\n",
        "        pl.col(\"request_id\"),\n",
        "        pl.lit(1).alias(\"rating\"),\n",
        "        pl.col(\"timestamp\"),\n",
        "    )\n",
        "    .sort(\"timestamp\")\n",
        ")\n",
        "\n",
        "train_models, _ = train_test_split(mlm_format_data, test_size=0.5, shuffle=False)\n",
        "\n",
        "# we want to find the requests, where user added at least one item to cart\n",
        "requests_for_mixigen_training = (\n",
        "    train_ranker_data\n",
        "    .filter(pl.col(\"action_type\") == \"AT_CartUpdate\")\n",
        "    .select(\"request_id\", \"user_id\")\n",
        "    .unique()\n",
        "    .filter(\n",
        "        pl.col(\"request_id\").is_not_null()\n",
        "        & pl.col(\"user_id\").is_not_null()\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTtufB3SCW5o"
      },
      "source": [
        "Train a set of 3-4 CG models (you can add your own, like random sampling, to experiment) and for each user in `user_for_mixigen_training` call each CG, log it's candidate list. Each item in list should contain `request_id`, `user_id`, `product_id`, `position_in_cg` and `source_cg`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9Y8w9n4CW5p"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"ALS\": ALS(),\n",
        "    # TODO: add your CG models here\n",
        "\n",
        "}\n",
        "\n",
        "cgs = {\n",
        "    # TODO: fill with ANN/KNN/etc CG instances\n",
        "}\n",
        "\n",
        "for model in models:\n",
        "    model.fit(train_models)\n",
        "\n",
        "    # TODO: train model and build CG from it\n",
        "\n",
        "\n",
        "mixigen_train = []\n",
        "\n",
        "for model in models:\n",
        "    for user_id, request_id in requests_for_mixigen_training.iter_rows():\n",
        "\n",
        "        # TODO: your code here\n",
        "\n",
        "        # model.extract_candidates(user)\n",
        "        pass\n",
        "\n",
        "mixigen_train = pl.DataFrame(mixigen_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R0ZDNwsCW5q"
      },
      "outputs": [],
      "source": [
        "assert \"position_in_cg\" in mixigen_train.columns\n",
        "assert \"source_cg\" in mixigen_train.columns\n",
        "assert \"request_id\" in mixigen_train.columns\n",
        "assert \"user_id\" in mixigen_train.columns\n",
        "assert \"product_id\" in mixigen_train.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOZe_ZiYCW5r"
      },
      "source": [
        "Then, for each sample you should join the user and item features, computed on `train_models` subset of data and score them using you ranker model. After that you can order them within each request by score and set 1 as target, if item is in top-N, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCjSTZalCW5r"
      },
      "outputs": [],
      "source": [
        "N = 30\n",
        "\n",
        "mixigen_targets = []\n",
        "\n",
        "# TODO: your code here\n",
        "\n",
        "mixigen_targets = pl.DataFrame(mixigen_targets)\n",
        "\n",
        "assert \"target\" in mixigen_targets.columns\n",
        "assert \"request_id\" in mixigen_targets.columns\n",
        "assert \"product_id\" in mixigen_targets.columns\n",
        "\n",
        "mixigen_train = (\n",
        "    mixigen_train\n",
        "    .join(mixigen_targets, on=(\"request_id\", \"product_id\"), how=\"left\")\n",
        "    .sort(\"request_id\")\n",
        ")\n",
        "\n",
        "# TODO: add cg or user features you want to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r45fCqwuCW5s"
      },
      "source": [
        "The dataset we created is actually huge - we do not need that much data for our tasks, our features are way too dumb for that much data. Let's undersample by requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuEIRFKqCW5t"
      },
      "outputs": [],
      "source": [
        "sampled_requests = mixigen_train[\"request_id\"].unique().sample(fraction=0.5)\n",
        "\n",
        "sampled_mixigen_train = (\n",
        "    mixigen_train\n",
        "    .filter(pl.col(\"request_id\").is_in(sampled_requests))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8gPdLAUCW5u"
      },
      "source": [
        "Now we can actually build the dataset for training and fit the model! We'll leave samples with up to N + 10 position in source CG, although they obviously won't be in top-N of ranking :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x0WUuz_CW5v"
      },
      "outputs": [],
      "source": [
        "x_train, x_test = train_test_split(sampled_mixigen_train, test_size=0.2, shuffle=True)\n",
        "x_train = x_train.sort(\"request_id\")\n",
        "x_test = x_test.sort(\"request_id\")\n",
        "\n",
        "\n",
        "train_pool = Pool(\n",
        "\n",
        "    # TODO: fill numerical features, cat features, target and feature names\n",
        "\n",
        "    # group is mandatory for ranking losses\n",
        "    group_id=x_train[\"request_id\"].cast(str).to_numpy(),\n",
        ")\n",
        "test_pool = Pool(\n",
        "\n",
        "    # TODO: fill numerical features, cat features, target and feature names\n",
        "\n",
        "    # group is mandatory for ranking losses\n",
        "    group_id=x_test[\"request_id\"].cast(str).to_numpy(),\n",
        ")\n",
        "\n",
        "model = CatBoostRanker(eval_metric=\"NDCG\", iterations=100)\n",
        "model.fit(train_pool, eval_set=test_pool, early_stopping_rounds=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PJL7NpuCW5w"
      },
      "source": [
        "We've trained the model! Now let's see if our assumption about the position in CG was correct - the score should decrease with bigger position.\n",
        "\n",
        "The reference picture looks something like that:\n",
        "\n",
        "<img src=\"mixigen.png\" alt=\"drawing\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKMmqm3fCW5x"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plotting_data = x_test.with_columns(\n",
        "    mixigen_score=model.predict(test_pool)\n",
        ").select(\"cg\", \"rank\", \"mixigen_score\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(plotting_data.limit(10000).to_pandas(), x=\"rank\", y=\"mixigen_score\", hue=\"cg\")\n",
        "plt.xlabel(\"Position in the candidate generator\")\n",
        "plt.ylabel(\"Blending score\")\n",
        "plt.legend(title=\"Model\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewZythyzCW5y"
      },
      "source": [
        "In order to implement a fancy class with k-way merging of the candidate generators, we actually have to rewrite the candgens to support iteratively returning results instead of making huge queries, and current implementations won't be efficient with this - that's why we are doing only the model part."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}